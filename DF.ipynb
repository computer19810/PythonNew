{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EasyEnsemble 通过对原始的数据集进行随机下采样实现对数据集进行集成.\n",
    "# EasyEnsemble 有两个很重要的参数: (i) n_subsets 控制的是子集的个数 and (ii) replacement 决定是有放回还是无放回的随机采样.\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "ee = EasyEnsemble(random_state=0, n_subsets=10)\n",
    "X_resampled, y_resampled = ee.fit_sample(X, y)\n",
    "sorted(Counter(y_resampled[0]).items())\n",
    "# [(0, 163), (1, 163), (2, 163)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\HT\\AppData\\Local\\Temp/ipykernel_11696/3283838803.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for col in tqdm_notebook(df.columns):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbc505bd34043e4b7ea91a38b9cbbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from tensorflow.keras.applications.resnet import \n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    " \n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "norm_size = 100\n",
    "#datapath = 'C:\\\\Users\\\\HT\\\\Desktop\\\\论文\\\\Back\\\\data\\\\train'\n",
    "EPOCHS = 30\n",
    "INIT_LR = 1e-3\n",
    "#labelList = []\n",
    "#dicClass = {'cat': 0, 'dog': 1}\n",
    "classnum = 2\n",
    "batch_size =1600\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# 1: 加载数据文件，查看数据信息\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head() # broadband 即可：0-离开，1-留存\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df = df.sample(frac=0.7, replace=False, weights=None, random_state=None, axis=0)\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder\n",
    "for col in tqdm_notebook(df.columns):\n",
    "    if df[col].dtype == \"object\":\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(list(df[col].values) + list(df_test[col].values))\n",
    "        df[col] = encoder.transform(list(df[col].values))\n",
    "        df_test[col] = encoder.transform(list(df_test[col].values))\n",
    "        \n",
    "df = df.reset_index() # 重置索引\n",
    "df_test = df_test.reset_index()\n",
    "def clean_top_cols(df):\n",
    "    new_cols = [col for col in df.columns if df[col].value_counts(dropna=False, normalize=True).values[0] > 1]\n",
    "    return new_cols\n",
    "df_cols = clean_top_cols(df)\n",
    "df_test_clos = clean_top_cols(df_test)\n",
    "cols_to_drop = list(set(df_cols + df_test_clos))\n",
    "if 'bad_good' in cols_to_drop : # 查看标签列是否在其中\n",
    "    cols_to_drop.remove('bad_good') # 删除标签列\n",
    "#print(\"原始的train shape : \", df)\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)\n",
    "#print(\"清理后的train shape : \", df.shape)\n",
    "\n",
    "y = df['bad_good'] # 标签\n",
    "X1 = df.iloc[:, 1:-1] # 客户 id 没有用，故丢弃 cust_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199700, 626)\n",
      "(199700, 625)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "X1 = X1.drop('guozhai_flag',axis=1)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('bad_good', data=X1)\n",
    "plt.title(\"Good & Bad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = np.expand_dims(X1, axis = 2)\n",
    "X = X1[0:100000]\n",
    "X = np.array(X1, dtype=\"int\")\n",
    "#print(X1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.resize(100000,25,25,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 25, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "#y = y.drop(9)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        1\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "99995    0\n",
      "99996    0\n",
      "99997    0\n",
      "99998    0\n",
      "99999    0\n",
      "Name: bad_good, Length: 100000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labelList = []\n",
    "y = y [0:100000]\n",
    "labelList = np.array(y,dtype=\"int\")\n",
    "\n",
    "#X = tf.cast(X, tf.float32)\n",
    "print(y)\n",
    "#y.resize(2009,142)\n",
    "#y = np.expand_dims(y, axis=2)\n",
    "#print(y.shape)\n",
    "\n",
    "imageArr = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 2)            4098        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 4617s 92s/step - loss: 0.4514 - accuracy: 0.9302 - val_loss: 0.9193 - val_accuracy: 0.1854\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.18540, saving model to weights_best_Reset50_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "50/50 [==============================] - 4577s 92s/step - loss: 0.1765 - accuracy: 0.9586 - val_loss: 0.1999 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.18540 to 0.95795, saving model to weights_best_Reset50_model.hdf5\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 4581s 92s/step - loss: 0.1778 - accuracy: 0.9584 - val_loss: 0.1762 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.95795\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 4542s 91s/step - loss: 0.1830 - accuracy: 0.9584 - val_loss: 0.1747 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.95795\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 4535s 91s/step - loss: 0.1801 - accuracy: 0.9585 - val_loss: 0.1749 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.95795\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 4555s 91s/step - loss: 0.1826 - accuracy: 0.9585 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.95795\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 4519s 90s/step - loss: 0.1790 - accuracy: 0.9585 - val_loss: 0.1752 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.95795\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 4560s 91s/step - loss: 0.1753 - accuracy: 0.9586 - val_loss: 0.1752 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.95795\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 4548s 91s/step - loss: 0.1777 - accuracy: 0.9585 - val_loss: 0.1787 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.95795\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 4529s 91s/step - loss: 0.1782 - accuracy: 0.9585 - val_loss: 0.1746 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.95795\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 4539s 91s/step - loss: 0.1893 - accuracy: 0.9586 - val_loss: 0.1761 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.95795\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 4533s 91s/step - loss: 0.1836 - accuracy: 0.9585 - val_loss: 0.1748 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.95795\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 4549s 91s/step - loss: 0.1782 - accuracy: 0.9585 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.95795\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 4773s 96s/step - loss: 0.1735 - accuracy: 0.9586 - val_loss: 0.1742 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.95795\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 4481s 90s/step - loss: 0.1745 - accuracy: 0.9586 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.95795\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 4427s 89s/step - loss: 0.1740 - accuracy: 0.9585 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.95795\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 4448s 89s/step - loss: 0.1743 - accuracy: 0.9586 - val_loss: 0.1749 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.95795\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 4428s 89s/step - loss: 0.1742 - accuracy: 0.9586 - val_loss: 0.1745 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.95795\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 4413s 88s/step - loss: 0.1771 - accuracy: 0.9585 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.95795\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 4431s 89s/step - loss: 0.1747 - accuracy: 0.9586 - val_loss: 0.1744 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.95795\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 4438s 89s/step - loss: 0.1748 - accuracy: 0.9585 - val_loss: 0.1748 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.95795\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 4426s 89s/step - loss: 0.1760 - accuracy: 0.9586 - val_loss: 0.1748 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.95795\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 4428s 89s/step - loss: 0.1742 - accuracy: 0.9586 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.95795\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 4725s 95s/step - loss: 0.1741 - accuracy: 0.9586 - val_loss: 0.1747 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.95795\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 4466s 89s/step - loss: 0.1736 - accuracy: 0.9586 - val_loss: 0.1743 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.95795\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 4394s 88s/step - loss: 0.1738 - accuracy: 0.9586 - val_loss: 0.1748 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.95795\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 4421s 88s/step - loss: 0.1739 - accuracy: 0.9585 - val_loss: 0.1745 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.95795\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 4409s 88s/step - loss: 0.1746 - accuracy: 0.9586 - val_loss: 0.1745 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.95795\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 4420s 88s/step - loss: 0.1761 - accuracy: 0.9585 - val_loss: 0.1745 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.95795\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 4407s 88s/step - loss: 0.1741 - accuracy: 0.9586 - val_loss: 0.1747 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.95795\n",
      "<keras.callbacks.History object at 0x0000015774B293A0>\n",
      "Now,we start drawing the loss and acc trends graph...\n",
      "We are done, everything seems OK...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, valX, trainY, valY = train_test_split(imageArr, labelList, test_size=0.2, random_state=42)\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "train_datagen = ImageDataGenerator(featurewise_center=True,\n",
    "                featurewise_std_normalization=True,\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator() # 验证集不做图片增强\n",
    "train_generator = train_datagen.flow(trainX, trainY, batch_size=batch_size, shuffle=True)\n",
    "val_generator = val_datagen.flow(valX, valY, batch_size=batch_size, shuffle=True)\n",
    "checkpointer = ModelCheckpoint(filepath='weights_best_Reset50_model.hdf5',\n",
    "               monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    " \n",
    "reduce = ReduceLROnPlateau(monitor='val_accuracy', patience=10,\n",
    "               verbose=1,\n",
    "               factor=0.5,\n",
    "               min_lr=1e-6)\n",
    " \n",
    "model = ResNet50(weights=None, classes=classnum)\n",
    "#   inputs = input((25,25,1)) \n",
    "    #it seems you're using (3,None,None) instead.    \n",
    "    #choose based on your \"data_format\", which by default is channels_last \n",
    "\n",
    "#outputs = Lambda(mean_subtract,output_shape=not_necessary_with_tensorflow)(inputs)\n",
    "optimizer = Adam(learning_rate=INIT_LR)\n",
    "#model= load_model(\"my_model_resnet.h5\")\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit_generator(train_generator,\n",
    "              steps_per_epoch=trainX.shape[0] / batch_size,\n",
    "              validation_data=val_generator,\n",
    "              epochs=EPOCHS,\n",
    "              validation_steps=valX.shape[0] / batch_size,\n",
    "              callbacks=[checkpointer, reduce],\n",
    "              verbose=1, shuffle=True)\n",
    "model.save('my_model_resnet.h5')\n",
    "\n",
    "print(history)\n",
    " \n",
    "loss_trend_graph_path = r\"WW_loss.jpg\"\n",
    "acc_trend_graph_path = r\"WW_acc.jpg\"\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"Now,we start drawing the loss and acc trends graph...\")\n",
    "# summarize history for accuracy\n",
    "fig = plt.figure(1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(acc_trend_graph_path)\n",
    "plt.close(1)\n",
    "# summarize history for loss\n",
    "fig = plt.figure(2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(loss_trend_graph_path)\n",
    "plt.close(2)\n",
    "print(\"We are done, everything seems OK...\")\n",
    "# #windows系统设置10关机\n",
    "#os.system(\"shutdown -s -t 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now,we start drawing the loss and acc trends graph...\n",
      "We are done, everything seems OK...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"Now,we start drawing the loss and acc trends graph...\")\n",
    "# summarize history for accuracy\n",
    "fig = plt.figure(1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(acc_trend_graph_path)\n",
    "plt.close(1)\n",
    "# summarize history for loss\n",
    "fig = plt.figure(2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(loss_trend_graph_path)\n",
    "plt.close(2)\n",
    "print(\"We are done, everything seems OK...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189766, 25, 25, 3)\n",
      "(80000, 25, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['CUST_ID'], axis=1)\n",
    "df_test = df_test.drop('guozhai_flag',axis=1)\n",
    "df_test = np.array(df_test, dtype=\"int\")\n",
    "df_test.resize(189766,25,25,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py:266 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer resnet50: expected shape=(None, 224, 224, 3), found shape=(None, 25, 25, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6720/257109759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 759\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    760\u001b[0m             *args, **kwds))\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\HT\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py:266 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer resnet50: expected shape=(None, 224, 224, 3), found shape=(None, 25, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "df_test_pred = model.predict(df_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "112ccbf03df3789b737d7516971948e1e09b833ea7af33f18c7286584c67d554"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
