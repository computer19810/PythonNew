{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\HT\\AppData\\Local\\Temp/ipykernel_15624/67583872.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for col in tqdm_notebook(df.columns):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d572fee489849a59f93c6b0d308fd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go \n",
      "It is end of part\n",
      "使用随机森林进行训练\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15624/67583872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"使用随机森林进行训练\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[0mstringIndexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"indexed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m   \u001b[0msi_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringIndexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m   \u001b[0mtrain_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m   \u001b[0mtrain_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ht\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\users\\ht\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ht\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.mllib.feature import HashingTF, IDF\n",
    "#from pyspark.ml.feature import HashingTF, IDF\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head() # broadband 即可：0-离开，1-留存\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder\n",
    "for col in tqdm_notebook(df.columns):\n",
    "    if df[col].dtype == \"object\":\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(list(df[col].values) + list(df_test[col].values))\n",
    "        df[col] = encoder.transform(list(df[col].values))\n",
    "        df_test[col] = encoder.transform(list(df_test[col].values))\n",
    "        \n",
    "df = df.reset_index() # 重置索引\n",
    "df_test = df_test.reset_index()\n",
    "def clean_top_cols(df):\n",
    "    new_cols = [col for col in df.columns if df[col].value_counts(dropna=False, normalize=True).values[0] > 0.7]\n",
    "    return new_cols\n",
    "df_cols = clean_top_cols(df)\n",
    "df_test_clos = clean_top_cols(df_test)\n",
    "cols_to_drop = list(set(df_cols + df_test_clos))\n",
    "if 'bad_good' in cols_to_drop : # 查看标签列是否在其中\n",
    "    cols_to_drop.remove('bad_good') # 删除标签列\n",
    "#print(\"原始的train shape : \", df)\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  appname = \"RandomForestClassifier\"\n",
    "  master =\"local[4]\"\n",
    "  conf = SparkConf().setAppName(appname).setMaster(master) #spark配置        \n",
    "  spark=SparkSession.builder.config(conf=conf).getOrCreate()#spark实例化\n",
    "   \n",
    "#读取数据\n",
    "  #data=spark.read.csv('train.csv',header=True)\n",
    "\n",
    "#构造训练数据集\n",
    "  print(\"Let's go \")\n",
    "  data = spark.createDataFrame(df)\n",
    "  print(\"It is end of part\")\n",
    "  #dataSet = data.na.fill('0').rdd.map(list)#用0填充空值  \n",
    "  #data = data.rdd.map()\n",
    "  trainData, testData= data.randomSplit([0.7, 0.3], seed=7)\n",
    "  trainingSet = trainData.rdd.map(lambda x:Row(label=x[4], features=Vectors.dense(x[5:3])))  \n",
    "  #train_num = trainingSet.count()\n",
    "  #print(\"训练样本数:{}\".format(train_num))\n",
    "  \n",
    "    \n",
    "#使用随机森林进行训练\n",
    "  print(\"使用随机森林进行训练\")\n",
    "  stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "  si_model = stringIndexer.fit(trainingSet)\n",
    "  train_tf = si_model.transform(trainingSet)\n",
    "  train_tf.show(5)  \n",
    "  rf = RandomForestClassifier(numTrees=100, labelCol=\"indexed\", seed=7)\n",
    "  rfModel = rf.fit(train_tf)\n",
    "    \n",
    "#输出模型特征重要性、子树权重\n",
    "  print(\"模型特征重要性:{}\".format(rfModel.featureImportances))\n",
    "  print(\"模型特征数:{}\".format(rfModel.numFeatures))\n",
    "   \n",
    "#预测测试集\n",
    "  testSet = testData.map(lambda x:Row(label=x[-1], features=Vectors.dense(x[:-1]))).toDF()\n",
    "  test_num=testSet.count()\n",
    "  print(\"测试样本数:{}\".format(test_num))  \n",
    "  si_model = stringIndexer.fit(testSet)\n",
    "  test_tf = si_model.transform(testSet)  \n",
    "  predictResult = rfModel.transform(test_tf)\n",
    "  predictResult.show(5)\n",
    "  spark.stop()\n",
    "  \n",
    "#将预测结果转为python中的dataframe\n",
    "  columns=predictResult.columns#提取强表字段\n",
    "  predictResult=predictResult.take(test_num)#\n",
    "  predictResult=pd.DataFrame(predictResult,columns=columns)#转为python中的dataframe\n",
    "  \n",
    "#性能评估\n",
    "  y=list(predictResult['indexed'])\n",
    "  y_pred=list(predictResult['prediction'])\n",
    "  y_predprob=[x[1] for x in list(predictResult['probability'])]\n",
    "  precision_score=metrics.precision_score(y, y_pred)#精确率\n",
    "  recall_score=metrics.recall_score(y, y_pred)#召回率\n",
    "  accuracy_score=metrics.accuracy_score(y, y_pred)#准确率\n",
    "  f1_score=metrics.f1_score(y, y_pred)#F1分数\n",
    "  auc_score=metrics.roc_auc_score(y, y_predprob)#auc分数\n",
    "  print(\"精确率:\",precision_score )#精确率\n",
    "  print(\"召回率:\",recall_score )#召回率\n",
    "  print(\"准确率:\",accuracy_score )#准确率\n",
    "  print(\"F1分数:\", f1_score)#F1分数\n",
    "  print(\"auc分数:\",auc_score )#auc分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "112ccbf03df3789b737d7516971948e1e09b833ea7af33f18c7286584c67d554"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
